{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"classification_mushroom_edibility/","title":"Predicting mushroom edibility using classification models","text":""},{"location":"classification_mushroom_edibility/#summary","title":"Summary","text":"<p> Problem: I wanted to predict whether a mushroom is edible using its physical characteristics.</p> <p> Solution: I trained several binary classification models (logistic regression, random forest, and k-NN) to predict mushroom edibility. I used AUC to compare the models. I also used random forest to identify the most useful predictors.</p> <p> Results: The best models were random forest and k-NN, which both achieved a 1.0 (perfect) test set AUC. The most useful predictors were gill size, gill spacing, and habitat.</p>"},{"location":"classification_mushroom_edibility/#r-code","title":"R Code","text":"<p> Link to R code </p>"},{"location":"classification_mushroom_edibility/#dataset-description","title":"Dataset Description","text":"<p>The dataset described 1,000 mushrooms, each of which was either poisonous or edible. All of the 9 predictors were categorical, and are described in the table below.</p> Variable Type Categories  Edibility (target variable)   Binary   edible, poisonous  Bruises Binary bruises, no bruises Gill size Binary broad, narrow Stalk shape Binary enlarging, tapering Cap surface Nominal fibrous, grooves, scaly, smooth Gill attachment Nominal attached, descending, free, notched Habitat Nominal grasses, leaves, meadows, paths, urban, waste, woods Veil color Nominal brown, orange, white, yellow Gill spacing Ordinal crowded, close, distant Number of rings Ordinal zero, one, two"},{"location":"classification_mushroom_edibility/#dataset-preparation","title":"Dataset Preparation","text":"<p>Encoding categorical variables: I converted each nominal variable into an unordered factor, and each ordinal variable into an ordered factor.</p> <p>Train-test split: I split the dataset into an 80% training set and a 20% test set. This was necessary because I wanted to estimate each model\u2019s performance on data it was not trained on (the test set).</p>"},{"location":"classification_mushroom_edibility/#exploratory-data-analysis","title":"Exploratory Data Analysis","text":"<p>Target variable distribution: In the dataset, 50% of the mushrooms were poisonous and the  remaining 50% were edible. </p> <p>Predictor-target distributions: </p> <p></p> <p>Proportion of poisonous mushrooms within each category of each predictor</p> <p>In the figure above, we see that all mushrooms with a 'waste' habitat, an orange veil color, or a brown veil color were edible.  </p> <p>Meanwhile, mushrooms with 0 rings, a narrow gill size, or a 'paths' habitat had the largest proportion of poisonous mushrooms.</p>"},{"location":"classification_mushroom_edibility/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p>All the classification models had hyperparameters, which are parameters that control each model's complexity and that are set before model training. To approximate the  best hyperparameters, I used grid search, 5-fold cross-validation, and AUC (using the <code>caret</code> package). </p> <p>For example, if I test 10 sets of hyperparameter values using 5-fold cross validation,  then I obtain 5 validation set AUC values for each set of hyperparameter  values. The best set of hyperparameter values has the highest average validation set AUC.</p> <p></p> <p>\\(M_i(h_j)\\) is the value of the validation set evaluation metric for the \\(j\\)th set of hyperparameter  values and the \\(i\\)th cross-validation iteration.</p>"},{"location":"classification_mushroom_edibility/#classification-models","title":"Classification Models","text":""},{"location":"classification_mushroom_edibility/#logistic-regression","title":"Logistic regression","text":"<p>Logistic regression assumes a linear relationship between the log odds of a mushroom being poisonous and the predictors. It can handle categorical predictors using one-hot encoding.  $$ \\log(\\frac{p(\\boldsymbol{x})}{1-p(\\boldsymbol{x})}) = \\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_q x_q $$ where \\(p(\\boldsymbol{x})\\) is the probability that mushroom \\(\\boldsymbol{x}\\) is poisonous, and \\(x_i\\) equals 1 or 0 (1 if mushroom \\(\\boldsymbol{x}\\) belongs to predictor category \\(i\\), 0 otherwise).</p> <p>The coefficients \\(\\boldsymbol{\\beta}\\) are calculated by minimizing the objective function. I used elastic-net regularization to prevent overfitting. Elastic-net adds a lasso penalty term and a ridge penalty term to the objective function, which limit the magnitudes of the coefficients. $$ \\underset{\\boldsymbol{\\beta}}{\\mathrm{argmin}}(\\mathrm{RSS} + \\lambda\\sum_{j=1}^{q}[\\alpha \\beta_j^2 + (1 - \\alpha) |\\beta_j|]) $$ where \\(\\mathrm{RSS}\\) is the residual sum of squares, \\(\\lambda\\) is a hyperparameter that controls the amount of regularization, and \\(\\alpha\\) is a hyperparameter that balances the amounts  of ridge and lasso regularization.</p> <p></p> <p>Logistic regression validation heatmap (the best hyperparameter values were \\(\\lambda\\) = 0 and \\(\\alpha\\) = 0.05)</p> <p>In the figure above, we see that logistic regression models with a large \\(\\alpha\\)  and a large \\(\\lambda\\) have a low average validation set AUC because they underfit the data.  This is because a larger \\(\\alpha\\) removes more predictors (more lasso regularization) and a larger \\(\\lambda\\)  penalizes the coefficient magnitudes more.</p>"},{"location":"classification_mushroom_edibility/#random-forest","title":"Random forest","text":"<p>A classification tree splits the predictor space into regions using recursive binary splitting, then assigns a predicted class (poisonous or edible) for all mushrooms in each new region. </p> <p>Since a single classification tree has high variance, a random forest returns the average of a set of classification trees, where each tree is trained on a bootstrap sample of the training set. To decrease the correlation between the trees, each split considers a different random subset \\(S\\) of the predictors. The \\(\\mathrm{mtry}\\) hyperparameter controls the size of \\(S\\).  $$ p(\\boldsymbol{x}) = \\frac{1}{B}\\sum_{b=1}^{B}\\hat{y}^b(\\boldsymbol{x}) $$ where \\(p(\\boldsymbol{x})\\) is the probability that mushroom \\(\\boldsymbol{x}\\) is poisonous, \\(B\\) is the number of trees, and \\(\\hat{y}^b(\\boldsymbol{x})\\) equals 1 or 0 (1 if the classification tree trained on bootstrap sample \\(b\\) predicts  \\(\\boldsymbol{x}\\) to be poisonous, 0 otherwise).</p> <p></p> <p>Random forest validation curve (the best hyperparameter value was \\(\\mathrm{mtry}\\) = 9)</p>"},{"location":"classification_mushroom_edibility/#k-nn","title":"k-NN","text":"<p>k-Nearest Neighbors (k-NN) is a non-parametric method, meaning it makes no assumptions about the shape of the decision boundary in the predictor space.  k-NN classifies each mushroom \\(\\boldsymbol{x}\\) using the edibilities of the \\(k\\) nearest  training set mushrooms to \\(\\boldsymbol{x}\\) in the predictor space. \\(k\\) is a hyperparameter.</p> <p>$$ p(\\boldsymbol{x}) = \\frac{1}{k}\\sum_{\\boldsymbol{x}_j\\in K(\\boldsymbol{x})}^{} y(\\boldsymbol{x}_j) $$ where \\(p(\\boldsymbol{x})\\) is the probability that mushroom \\(\\boldsymbol{x}\\) is poisonous,  \\(K(\\boldsymbol{x})\\) is the set of the \\(k\\) nearest training set observations to \\(\\boldsymbol{x}\\) in  the predictor space, and \\(y(\\boldsymbol{x}_j)\\) equals 1 or 0 (1 if mushroom \\(\\boldsymbol{x}_j\\)  is poisonous, 0 otherwise).</p> <p></p> <p>k-nearest neighbors validation curve (the best hyperparameter value was \\(k\\) = 4)</p> <p>In the figure above, we see that k-NN models with a large \\(k\\) have a low average validation set  AUC because they underfit the data. This is because a larger \\(k\\) creates a smoother decision boundary.</p>"},{"location":"classification_mushroom_edibility/#model-evaluation","title":"Model Evaluation","text":"<p>To compare the models across all probability thresholds, I used their AUC (Area Under Curve) values. We can use AUC because the distribution of the target variable  in the dataset is balanced. The best model has the largest AUC value (0.5 \u2264 AUC \u2264 1).</p> <p></p> <p>Test set AUCs for all classification models</p> <p>In the figure above, we observe that random forest and k-NN achieved 1.0 (perfect) test set AUCs. This means that for every poisonous mushroom in the test set, these models predicted  a higher probability of being poisonous than for all the edible mushroom in the test set.</p>"},{"location":"classification_mushroom_edibility/#model-interpretation","title":"Model Interpretation","text":"<p>In a classification tree, a pure node is a node that only contains poisonous mushrooms, or  only contains edible mushrooms. The Gini index measures the purity of a node in a classification tree.  A smaller Gini index value indicates higher purity.  $$ G(A) = 1 - [p_{\\mathrm{poisonous}}^2(A) + p_{\\mathrm{edible}}^2(A)] $$ where \\(G(A)\\) is the Gini index of node \\(A\\), and \\(p_{\\mathrm{poisonous}}(A)\\) is the proportion of poisonous mushrooms in \\(A\\).</p> <p>Splitting a node \\(A\\) returns a left child node (\\(A_{\\mathrm{left}}\\)) and a right child node (\\(A_{\\mathrm{right}}\\)),  both of which have a smaller Gini index than \\(A\\). $$ \\Delta G(A) = G(A) - [\\frac{n(A_{\\mathrm{left}})}{n(A)}G(A_{\\mathrm{left}})+\\frac{n(A_{\\mathrm{right}})}{n(A)}G(A_{\\mathrm{right}})] $$ where \\(\\Delta G(A)\\) is the decrease in Gini index from splitting node \\(A\\), and \\(n(A)\\) is the number of  mushrooms in \\(A\\).</p> <p>Within a random forest, the larger the average decrease in the Gini index for all  splits using predictor P, the more useful P is.</p> <p></p> <p>Random forest predictor importances of the five most important predictors </p>"},{"location":"classification_mushroom_edibility/#conclusion","title":"Conclusion","text":"<p>The best models were random forest and k-NN, which both achieved a 1.0 (perfect) test set AUC. The most useful predictors were gill size, gill spacing, and habitat.</p>"},{"location":"knime_etl/","title":"Automating Tilray's weekly ETL using KNIME and SQL Server","text":""},{"location":"knime_etl/#summary","title":"Summary","text":"<p> Problem: Every week, the analytics team used Power Query to transform and import new sales and inventory data (stored in over 30 Excel files) into their Power BI reports. After a few years, the Power BI report data refreshes became very slow, because Power Query was transforming data from thousands of Excel files during each data refresh.</p> <p> Solution: I used KNIME to develop an ETL program to extract the data from the Excel files, transform the data, then load the transformed data into SQL Server. This allowed the team to import the data from SQL Server instead of from thousands of Excel files.</p> <p> Results: The Power BI report data refreshes became 80% faster.</p>"},{"location":"knime_etl/#knime","title":"KNIME","text":"<p>KNIME is a low-code data processing application where you can chain \u201cnodes\u201d to create programs. A node is like a function in a programming language. </p> <p></p> <p>Screenshot of KNIME.</p>"},{"location":"knime_etl/#definitions","title":"Definitions","text":"<code>f</code> An Excel file containing sales and inventory data for one week.  <code>f.data</code> The data table extracted from <code>f</code>. All columns in <code>f.data</code> are the string data type.  <code>T</code> <p>A sequence of data transformations. These include:  </p> <ul> <li>Transforming column contents using regular expressions.  </li> <li>Converting column data types.  </li> </ul> <code>T(f.data)</code> The resulting table after applying <code>T</code> to <code>f.data</code>.  <code>all_data</code> A database table that will store <code>T(f.data)</code> for every Excel file <code>f</code>. match A table <code>A</code> matches a table <code>B</code> if <code>A</code> and <code>B</code> have the same column names and data types. <code>T(f.data)</code> can only be loaded into <code>all_data</code> if <code>T(f.data)</code> matches <code>all_data</code>. <code>all_file_names</code> A set of all the Excel file names. <code>loaded_file_names</code> A set of the Excel file names <code>f.name</code> such that <code>T(f.data)</code> has been loaded into <code>all_data</code>."},{"location":"knime_etl/#pseudocode-of-the-etl-program-similar-to-python","title":"Pseudocode of the ETL program (similar to Python)","text":"<pre>\nAlgorithm ETL_program(file_names):  \n    Input:   \n        file_names: A set of Excel file names.\n    Output: A dictionary of (file name, error message) items. \n\n    not_loaded_file_names = file_names \u2212 loaded_file_names. \n    file_name_to_error_message = dict().\n\n    for each Excel file name f.name in not_loaded_file_names:\n        f.data = Extract the data table from f.  \n        try:\n            T(f.data) = Apply each transformation in T to f.data. \n            Load T(f.data) into all_data.\n        except Exception as error:\n            file_name_to_error_message[f.name] = str(error).   \n        else: \n            Add f.name to loaded_file_names.\n\n    return file_name_to_error_message.\n\n</pre> <p> After executing <code>file_name_to_error_message = ETL_program(all_file_names)</code>, I execute the following steps manually:</p> <pre>\nfor f.name, error_message in file_name_to_error_message.items():\n    Use error_message to update the data transformations in T. \n    Execute ETL_program(set(f.name)). \n</pre>"},{"location":"knime_etl/#example-execution-of-the-etl-program","title":"Example execution of the ETL program","text":"<code>T</code>: <ol> <li>Convert <code>date</code> to the date data type.    </li> <li>Convert <code>column_a</code> to the integer data type.   </li> </ol> <p>1. Execute <code>ETL_program(set(f_1.name))</code>: </p> <p></p> <p><code>T(f_1.data)</code> can't be loaded into <code>all_data</code> because <code>\"missing\"</code> in <code>column_a</code> can't be converted to an integer. </p> <p>2. Update <code>T</code>: </p> <code>T</code>: <ol> <li>Convert <code>date</code> to the date data type.    </li> <li>In <code>column_a</code>, replace all cells matching the regular expression <code>\u201c^missing$\u201d</code> with <code>null</code>.</li> <li>Convert <code>column_a</code> to the integer data type.</li> </ol> <p>3. Execute <code>ETL_program(set(f_1.name))</code>: </p> <p></p> <p>After updating <code>T</code>, <code>T(f_1.data)</code> is loaded into <code>all_data</code>.</p>"},{"location":"power_bi_python/","title":"Creating Tilray's Power BI reports using Python","text":""},{"location":"power_bi_python/#summary","title":"Summary","text":"<p> Problem: The analytics team needed to create hundreds of Power BI report pages, each presenting the result of a different database query. </p> <p> Solution: I wrote a Python script to automate the creation of the Power BI report pages by editing the files in a Power BI Project folder. </p> <p> Results: The Python script saved days of work for my team. Without the script, we would have had to manually create the report pages using the Power BI graphical user interface (GUI). </p>"},{"location":"power_bi_python/#power-bi-projects-pbip","title":"Power BI Projects (PBIP)","text":"<p>A Power BI Project (PBIP) defines a Power BI report using a folder of plain text files (PBIP documentation). By using PBIPs, we can edit Power BI reports using a programming language, such as Python.     </p> project/<pre><code> \ud83d\udcc2.SemanticModel\n \ud83d\udcdc.pbip\n \ud83d\udcc2.Report\n</code></pre>"},{"location":"power_bi_python/#semanticmodel-folder","title":"<code>.SemanticModel</code> folder","text":"<p>Defines the data tables that are used to create the data visualizations in the report pages. </p> project/.SemanticModel/<pre><code> \ud83d\udcc2tables # (1)!\n \u2523 \ud83d\udcdctable_1.tmdl\n \u2517 \ud83d\udcdctemplate_table.tmdl\n</code></pre> <ol> <li>Each <code>.tmdl</code> file in the <code>tables</code> folder defines a semantic model table. For example, the file contains the Power Query M code that produces the table. </li> </ol>"},{"location":"power_bi_python/#pbip-file","title":"<code>.pbip</code> file","text":"<p>A Power BI report such that changes made to the <code>.Report</code> and <code>.SemanticModel</code> folders will change the <code>.pbip</code> file. Furthermore, changes made to the <code>.pbip</code> file using the Power BI GUI will change the <code>.Report</code> and <code>.SemanticModel</code> folders. </p> <p>I used the Power BI GUI to create a <code>template_table</code> report page, which presents the data from the <code>template_table</code> semantic model table. The <code>template_table</code> report page defines the appearance of the report pages that the Python script will create.  </p> <p></p> <p><code>project.pbip</code></p>"},{"location":"power_bi_python/#report-folder","title":"<code>.Report</code> folder","text":"<p>Defines all the report pages in the Power BI report.  </p> project/.Report/<pre><code> \ud83d\udcc2definition\n \u2523 \ud83d\udcc2pages # (1)!\n \u2503 \u2523 \ud83d\udcc2template_table\n \u2503 \u2503 \u2523 \ud83d\udcc2visuals\n \u2503 \u2503 \u2503 \u2517 \ud83d\udcc2table_visual\n \u2503 \u2503 \u2503 \u2503 \u2517 \ud83d\udcdcvisual.json # (2)!\n \u2503 \u2503 \u2517 \ud83d\udcdcpage.json\n</code></pre> <ol> <li>Each folder in the <code>pages</code> folder defines a report page. </li> <li>Each <code>visual.json</code> file defines a data visualization. For example, the file specifies the type of data visualization, and which semantic model tables to use to create the visualization. </li> </ol>"},{"location":"power_bi_python/#python-code","title":"Python code","text":"<p>I used the following Python packages: </p> <ul> <li> <code>pathlib</code> to represent filesystem paths. </li> <li> <code>shutil</code> to copy files and folders.</li> <li> <code>re</code> to parse <code>.tmdl</code> files using regular expressions. </li> <li> <code>json</code> to edit <code>.json</code> files. </li> <li> <code>typing</code> to add type hints.   </li> </ul> <p>1. Create variables. </p> <p>add_pages_to_report.py<pre><code>project_path = Path(r\"\") # paste the path to the Power BI project folder \npages_path = project_path / \".Report\" / \"definition\" / \"pages\"\ntables_path = project_path / \".SemanticModel\" / \"definition\" / \"tables\"\n\nnew_table_name = \"table_1\"\nnew_page_name = new_table_name\n</code></pre> 2. Extract the column names of each semantic model table from the <code>.tmdl</code> files. </p> add_pages_to_report.py<pre><code>def get_column_names() -&gt; Dict[str, List[str]]:\n    table_name_to_column_names = dict()\n    for table_tmdl_path in tables_path.glob('*.tmdl'):\n        table_name = (re\n                      .search(r'.*\\\\(.*)\\.tmdl$', str(table_tmdl_path))\n                      .group(1)\n                      )\n        with open(table_tmdl_path, 'r') as f:\n            table_tmdl_data = f.read()\n        column_names = re.findall(r'\\n\\tcolumn (.*)\\n', table_tmdl_data)\n        table_name_to_column_names[table_name] = column_names\n    return table_name_to_column_names\n\n\ntable_name_to_column_names = get_column_names()\n</code></pre> table_1.tmdltable_name_to_column_names project/.SemanticModel/definition/tables/table_1.tmdl<pre><code>column d\n    dataType: int64\n    formatString: 0\n    lineageTag: d8b5b183-7b7a-47a0-8219-211965773f6d\n    summarizeBy: none\n    sourceColumn: d\n\n    annotation SummarizationSetBy = User\n\ncolumn e\n    dataType: int64\n    formatString: 0\n    lineageTag: c55162d8-0445-42ef-9b16-236054053d6d\n    summarizeBy: none\n    sourceColumn: e\n\n    annotation SummarizationSetBy = User\n</code></pre> <pre><code>{\n'template_page': ['a', 'b', 'c'],\n'table_1': ['d', 'e', 'f', 'g']\n}\n</code></pre> <p> 3. Create a copy of the <code>template_table</code> folder named <code>table_1</code>. </p> add_pages_to_report.py<pre><code>def copy_template_page(new_page_name: str) -&gt; None:\n    template_page_path = pages_path / \"template_table\"\n    new_page_path = pages_path / new_page_name\n    shutil.copytree(template_page_path, new_page_path)\n\n\ncopy_template_page(new_page_name)\n</code></pre> beforeafter project/.Report/definition/pages/<pre><code>\ud83d\udcc2template_table\n</code></pre> project/.Report/definition/pages/<pre><code>\ud83d\udcc2table_1\n\ud83d\udcc2template_table\n</code></pre> <p> 4. Edit <code>page.json</code> to change the name of the new report page from <code>template_table</code> to <code>table_1</code>.  </p> add_pages_to_report.py<pre><code>def edit_page_json(page_name: str) -&gt; None:\n    page_json_path = pages_path / page_name / \"page.json\"\n    with open(page_json_path, 'r') as f:\n        page_json_data = json.load(f)\n\n    page_json_data['name'] = page_name\n    page_json_data['displayName'] = page_name\n\n    with open(page_json_path, 'w') as f:\n        json.dump(page_json_data, f, indent=4)\n\n\nedit_page_json(new_page_name)\n</code></pre> beforeafter project/.Report/definition/pages/table_1/page.json<pre><code>\"name\": \"template_table\",\n\"displayName\": \"template_table\",\n</code></pre> project/.Report/definition/pages/table_1/page.json<pre><code>\"name\": \"table_1\",\n\"displayName\": \"table_1\",\n</code></pre> <p> 5.  Edit <code>visual.json</code> to present the columns from <code>table_1</code> instead of the columns from <code>template_table</code>. </p> add_pages_to_report.py<pre><code>visual_json_column_template = ({\n    \"field\": {\n        \"Column\": {\n            \"Expression\": {\n                \"SourceRef\": {\n                    \"Entity\": \"table_name\"\n                }\n            },\n            \"Property\": \"column_name\"\n        }\n    },\n    \"queryRef\": \"table_name.column_name\",\n    \"nativeQueryRef\": \"column_name\"\n})\n\n\ndef create_visual_json_columns(table_name: str) -&gt; str:\n    visual_json_columns = []\n    for column_name in table_name_to_column_names[table_name]:\n        visual_json_column = copy.deepcopy(visual_json_column_template)\n        (visual_json_column['field']['Column']['Expression']\n         ['SourceRef']['Entity']) = table_name\n        visual_json_column['field']['Column']['Property'] = column_name\n        visual_json_column['queryRef'] = f'{table_name}.{column_name}'\n        visual_json_column['nativeQueryRef'] = column_name\n        visual_json_columns.append(visual_json_column)\n    return visual_json_columns\n\n\ndef edit_visual_json(page_name: str, table_name: str) -&gt; None:\n    visual_json_path = (pages_path / page_name /\n                        \"visuals\" / \"table_visual\" / \"visual.json\")\n    with open(visual_json_path, 'r') as f:\n        visual_json_data = json.load(f)\n\n    (visual_json_data['visual']['query']['queryState']['Values']\n     ['projections']) = create_visual_json_columns(table_name)\n\n    with open(visual_json_path, 'w') as f:\n        json.dump(visual_json_data, f, indent=4)\n\n\nedit_visual_json(new_page_name, new_table_name))\n</code></pre> beforeafter project/.Report/definition/pages/table_1/visuals/line_graph/visual.json<pre><code>{\n    \"field\": {\n        \"Column\": {\n            \"Expression\": {\n                \"SourceRef\": {\n                    \"Entity\": \"template_table\"\n                }\n            },\n        \"Property\": \"a\"\n        }\n    },\n\"queryRef\": \"template_table.a\",\n\"nativeQueryRef\": \"a\"\n},\n</code></pre> project/.Report/definition/pages/table_1/visuals/line_graph/visual.json<pre><code>{\n    \"field\": {\n        \"Column\": {\n            \"Expression\": {\n                \"SourceRef\": {\n                    \"Entity\": \"table_1\"\n                }\n            },\n        \"Property\": \"d\"\n        }\n    },\n\"queryRef\": \"table_1.d\",\n\"nativeQueryRef\": \"d\"\n},\n</code></pre>"},{"location":"power_bi_python/#new-report-pages","title":"New report pages","text":"beforeafter <p><code>project.pbip</code></p> <p></p> <p><code>project.pbip</code></p>"}]}